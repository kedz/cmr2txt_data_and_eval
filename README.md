# cmr2txt_data_and_eval

This repository contains model outputs for the E2E Challenge and Viggo NLG datasets created for the paper 
[Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies](https://aclanthology.org/2020.emnlp-main.419/).

# Obtaining validation and test set data.

To access the data unzip the following files:
- e2e_test_model_outputs.zip
- e2e_valid_model_outputs.zip
- viggo_model_outputs.zip

If you run: 

```shell
unzip viggo_model_outputs.zip
unizp e2e_valid_model_outputs.zip
unzip e2e_test_model_outputs.zip
```

this should create the following directory structure:

```shell
model_outputs/{E2E,Viggo}/{BART,bi-gru,transformer}/{valid,test}
```

with each directory containing a jsonl files of model outputs (a jsonl file is a text file where is line is a new json object).
The key to interpretting the json file names is the following:
```{dataset}.{data-partition}.{arch}.{linearization}.{dataaug}.{randomseed}.jsonl```
where:
- `dataset` is either E2E or Viggo
- `data-partition` - validation or test set
- `arch` - BART, transformer, or bi-gru
- `linearization` - the linearization strategy and plan generation used.
    - `rnd` - Random linearization, no plan generation used.
    - `if` - Increasing Frequency linearization, no plan generation used.
    - `fp` - Fixed Position linearization, no plan generation used.
    - `at_bgup` - Alignment Training, plans generated using the Bigram Utterance Planner.
    - `at_nup` - Alignment Training, plans generated using the Neural Utterance Planner.
    - `at_oracle` - Alignment Training, plans generated using a human reference oracle.
- `dataaug` - whether synthetic data was added to the model's training data.
    - `b` indicates only the original training data is used. 
    - `bp` indicates the phrase based  dataugmentation is used to add aditional training examples.
- `randomseed` - each model was trained and evaluated 5 different times using distinct random seeds.


# Data Schema

Each model output example (line in a jsonl file) has the following schema:
- `"mr"` - a dictionary containing the input meaning representation, i.e. dialog act (da) and slot values from which to generate an utterance
-  `"input"` - the meaning representation as it was turned into a flat sequence by the linearization strategy. This is what is fed into the actula seq2seq NLG model.
- `"input_slot_fillers"` - the input sequence with the dialog act (and rating on the Viggo dataset) removed.
- `"references"` - a string containing the human references for this example. Multiple references are delimited by a new line `"\n"` character.  
- `"outputs"` - the beam search outputs of the model.
- `"reranked_beam_output_index"` - the index of outputs that was picked by the beam reranker. This is beam index of the output used for evaluation.



Each output in `"outputs"` list has the following schema:
- `"tokens"` - a list of tokens generated by the model.
- `"pretty"` - the pretty string of the tokens generated by the model. The tokens were detokenized, and any placeholder tokens replaced with their actual values.
- `"tags"` - the tag sequence of the rule based tagger for parsing the actual meaning representation of the generated utterance.
- `"pred_lmr"` - the predicted linearize meaning representation implied by the tag sequence. After human correction, this is used to evaluate semantic error rate and the order acccuracy.
- `"mean_log_prob"` - the mean log probability of the generated tokens.
- `"err"` - the number of semantic errors (without human correct) which were used by the beam reranker.
- `"beam_candidate_num"` - rank of this output in the beam
- `"reranked_beam_candidate_num"` - rank of this output in the reranked beam.

To iterate through the outputs of the a file use the following code snippet:

```python
import pathlib
import json

path = pathlib.Path("PATH/TO/JSONL/FILE")

with path.open("r") as fh:
    for line in fh:
        example = json.loads(line)
        output = example["outputs"][example["reranked_beam_output_index"]]
        print("LINEARIZED MR INPUTS")
        print(output["input"])
        print("MODEL GENERATED TOKENS")
        print(output["tokens"])
        print("MODEL GENERATED PRETTY STRING")
        print(output["pretty"])
        print()
```


